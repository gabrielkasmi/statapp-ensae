{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation d'un réseau de neurones avec Numpy\n",
    "\n",
    "Ce notebook vous propose d'implémenter votre propre réseau de neurones dans Numpy et de l'entrainer à reconnaitre des chiffres. On commencera avec une sigmoide, puis on généralisera à un perceptron multi couches. \n",
    "\n",
    "L'objectif est de coder toutes les briques nécessaires à la définition et à l'entrainement du modèle par descente de gradient (stochastique).\n",
    "\n",
    "<i> Repris et adapté d'un TD de Francois Pierre Paty </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.datasets import load_digits # chargement qui peut être un peu long.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Dans un premier temps, on charge et on prépare les données. \n",
    "\n",
    "### Chargement et visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits() # chargement du jeu de données\n",
    "\n",
    "# affichage de quelques exemples \n",
    "\n",
    "#seed pour la reproducibilité\n",
    "random.seed(42)\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize = (18, 4)) # une ligne de 5 vignettes pour afficher des échantillons\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    # tirer une image dans la base de données\n",
    "    sample_index = np.random.randint(0,digits.images.shape[0])\n",
    "    sample_image = digits.images[sample_index]\n",
    "    ax[i].imshow(sample_image, cmap = \"gray_r\", interpolation = \"nearest\")\n",
    "\n",
    "\n",
    "# caractéristiques des données pour savoir avec quels objets on travaille\n",
    "print(type(sample_image))\n",
    "print(sample_image.shape)\n",
    "print(np.min(sample_image), np.max(sample_image))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont des matrices $8\\times 8$. La valeur de chaque entrée varie entre 0 et 16. Cette valeur correspond à l'intensité du pixel. \n",
    "\n",
    "### Préparation des données\n",
    "\n",
    "A présent, on sépare les données entre une base d'entrainement et une base de test. On normalize également les données. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(digits.data, dtype='float32')\n",
    "labels = np.asarray(digits.target, dtype='int32') # correspond aux labels (i.e. chiffres)\n",
    "\n",
    "test_size = 0.25 # proportion de la base de données qu'on consacre au test \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size)\n",
    "\n",
    "# mean = 0 and standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles \n",
    "\n",
    "\n",
    "### Préliminaires\n",
    "\n",
    "Dans cette section, on implémente une régression logistique que l'on entraine par descente de gradient. Les objectifs sont:\n",
    "\n",
    "* Implémenter un modèle <i> forward </i> simple, sans couche cachée (a.k.a. régression logistique) de la forme \n",
    "  \n",
    "  $$\n",
    "  y = \\sigma\\left(\\mathbf{W} x + b\\right)\n",
    "  $$\n",
    "où $\\sigma(x)$ est la [fonction softmax](https://fr.wikipedia.org/wiki/Fonction_softmax).\n",
    "\n",
    "* Construire une fonction de prédiction qui retourne le label $y$ le plus probable pour un input $x$ donné\n",
    "* Construire une fonction de précision pour un batch d'inputs $\\mathbf{X}$ et leurs outputs correspondants $\\mathbf{y}$.\n",
    "* Construire le gradient qui calcule $\\frac{d}{dW} -\\log(\\sigma(W x + b))$ pour un $x$ et un $y$ donnés\n",
    "* Constuire une fonction d'entrainement qui utilise le gradient pour mettre à jour les poids $W$ et $b$.\n",
    "\n",
    "#### Encodage one-hot des labels\n",
    "\n",
    "Avant de commencer, on va définir une fonction qui calcule l'encodage one-hot pour un nombre de classes données. Avec cette fonction, on représente les labels des $K$ classes par un vecteur $y\\in\\{0,1\\}^K$, où la $k$ème coordonnée vaut 1 si le label de l'input correspond à la classe $k$, et 0 partout ailleurs. Par exemple, si on a trois classes \"Bleu\", \"Blanc\", \"Rouge\", alors on encode les labels des inputs qui appartiennent à la classe \"Bleu\" comme suit : $(1, 0, 0)$ ; ceux de la classe blanc $(0, 1, 0)$ et ceux de la classe rouge $(0, 0, 1)$. \n",
    "\n",
    "Enfin, si on a $n$ entrée, cette fonction permet de définir une matrice $n\\times K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    #\n",
    "    # A compléter\n",
    "    #\n",
    "\n",
    "one_hot(n_classes=10, y =[1,3,8]) # la longueur de y nous donne le nombre d'échantillons dans le batch.\n",
    "\n",
    "# test de la fonction. Si à l'exécution la cellure renvoie une AssertionError, alors la fonction n'a pas été codée\n",
    "# comme attendu.\n",
    "test = np.array([\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]\n",
    "])\n",
    "\n",
    "assert one_hot(n_classes=10, y = [1,3,8]).all() == test.all(), \"Erreur dans la définition de la fonction one-hot.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La fonction softmax\n",
    "\n",
    "On implémente maintenant la fonction softmax vectorielle\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "où la normalisation est faite par coordonnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    #\n",
    "    # A compléter\n",
    "    #\n",
    "\n",
    "print(\"Softmax of a single vector:\")\n",
    "print(softmax([10, 2, -3]))\n",
    "print('')\n",
    "\n",
    "print(\"Softmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))\n",
    "\n",
    "\n",
    "test = np.array([\n",
    "    [9.99662391e-01, 3.35349373e-04, 2.25956630e-06],\n",
    "    [2.47262316e-03, 9.97527377e-01, 1.38536042e-11],\n",
    "])\n",
    "\n",
    "assert softmax(X).all() == test.all(), \"Softmax ill-defined\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonction de perte\n",
    "\n",
    "Avant de pouvoir définir le modèle, on implémente la fonction de perte qui étant donné un vecteur de labels `y_true` encodé en \"one-hot\" et des probabilités prédites `y_pred` retourne la log-vraisemblance (négative).\n",
    "\n",
    "La fonction est de la forme suivante:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y_{pred}, y_{true}) := \\frac{1}{N}\\sum_{i=1}^N \\ell\\left(y_{true}^{(i)},y_{true}^{(i)}\\right)\n",
    "$$\n",
    "Où $y^{(i)}\\in\\mathbb{R}^{10}$\n",
    "\n",
    "On se donne comme fonction de perte la [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy). On peut mettre en avant le fait que dans ce cas, la maximisation de la vraisemblance est équivalente à la minimisation de la cross-entropy. La maximisation de la vraisemblance se fait par rapport à des poids $\\theta$, qui correspondent ici à $W$ et $b$. La cross-entropy pour un échantillon est donnée par:\n",
    "\n",
    "$$\n",
    "\\ell\\left(y_{true}^{(i)}, y_{pred}^{(i)}\\right) = y_{true}^{(i)}\\log\\left(y_{pred}^{(i)}\\right) - \\left(1 - y_{true}^{(i)}\\right)\\log\\left(1 - y_{pred}^{(i)}\\right)\n",
    "$$\n",
    "\n",
    "Que l'on implémente de la manière suivante: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred) # permet de s'assurer qu'il y a toujours deux dimensions. Un array (,n) sera modifié en (1,n)\n",
    "    loglikelihood = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis = 1)\n",
    "    return -np.mean(loglikelihood)\n",
    "\n",
    "# on peut vérifier que la NLL de deux prédictions proches est presque parfaite:\n",
    "\n",
    "Y_true = np.array([\n",
    "    [0,1,0],\n",
    "    [1,0,0],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "Y_pred = np.array([\n",
    "    [0,1,0],\n",
    "    [.99, .01, 0],\n",
    "    [0,0,1]\n",
    "])\n",
    "\n",
    "nll(Y_true, Y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique\n",
    "\n",
    "Dans la cellule suivante, on définit une [classe python](https://docs.python.org/fr/3/tutorial/classes.html) qui correspond à un modèle linéaire pouvant être entrainé par descente de gradient stochastique. Pour l'instant, on considère que l'on entraine le modèle échantillon par échantillon. \n",
    "\n",
    "On appelle cette classe `LogisticRegression`. Elle comprend les méthodes suivantes:\n",
    "* Une initialisation (`__init__`) - obligatoire en Python qui permet de créer un objet de cette classe. Les arguments de `__init__` sont la taille de l'input et la taille de l'output. Cette initialisation permet de définir les poids du modèle. \n",
    "* Une fonction `forward` qui prend en entreé un vecteur de d'inputs et renvoie son softmax.\n",
    "* Une fonction `predict` qui renvoie l'argmax du softmax renvoyé par `forward`.\n",
    "* Une fonction `grad_loss` qui retourne le gradient de la perte, étant donnés un `x` et un label `y_true`.\n",
    "* Une fonction `train` qui met à jour les poids par SGD. Cette fonction prend en argument le `learning_rate` en plus des inputs et des outputs. \n",
    "* Une fonction `loss` qui calcule la perte\n",
    "* Une fonction `accuracy` qui calcule la précision du modèle.\n",
    "\n",
    "\n",
    "L'apprentissage se fait avec la fonction `train`. Les poids du modèle sont mis à jour par descente de gradient, selon la formule:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\nabla \\mathcal{L}_{\\theta_t}(y_{pred}, y_{true})\n",
    "$$\n",
    "L'hyperparamètre $\\eta$ est le <i> [learning rate](https://en.wikipedia.org/wiki/Learning_rate) </i>. C'est un hyperparamètre à ajuster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        L'initialisation permet de définir les poids du modèle W et b. \n",
    "        Ces poids sont initialisés de manière aléatoire (peu importe le choix de l'initialisation)\n",
    "        Il ne faut surtout pas initialiser W à 0.\n",
    "        \"\"\"\n",
    "        self.W = #### A compléter\n",
    "        self.b = #### A compléter\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode calcule le softmax de X*W + b\n",
    "        \"\"\"\n",
    "        Z = ### A compléter\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode retourne la prédiction en fonction des softmax. \n",
    "        elle renvoie l'argmax (i.e. l'indice) associé à la probabilité \n",
    "        prédite la plus élevée.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis = 1)\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        \"\"\"\n",
    "        Cette méthode calcule les gradients et les stocke dans un dictionnaire `grads`\n",
    "        qu est renvoyé lorsqu'on appelle la méthode.\n",
    "        \"\"\"\n",
    "        y_pred      = #### A compléter \n",
    "        dnll_output = #### A compléter \n",
    "        grad_W      = #### A compléter \n",
    "        grad_b      = #### A compléter\n",
    "        grads  = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Cette fonction met à jour les poids du modèle selon la formule de la descente de gradient. \n",
    "        \"\"\"\n",
    "        grads  = self.grad_loss(x, y)\n",
    "        self.W = #### A compléter \n",
    "        self.b = #### A compléter \n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcule la perte\n",
    "        \"\"\"\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Calcule la précision\n",
    "        \"\"\"\n",
    "        y_preds = np.argmax(self.forward(X), axis = 1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer, on peut définir un modèle et le tester sur un échantillon sans entrainement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Définition d'une fonction qui affiche les outputs\n",
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    \"\"\"\n",
    "    Affiche la prédiction du modèle pour un échantillon donné et un \n",
    "    nombre de classe donné.\n",
    "    \"\"\"\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()\n",
    "    \n",
    "# définition du modèle\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "model = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = model.loss(X_train, y_train)\n",
    "train_acc = model.accuracy(X_train, y_train)\n",
    "test_acc = model.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\" % (train_loss, train_acc, test_acc))\n",
    "\n",
    "plot_prediction(model, sample_idx=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut à présent entrainer le modèle. Lorsqu'on entraine un modèle, on va effectuer un certain nombre d'itérations sur le dataset d'entrainement. Ces itérations sont les `epochs`. Couplées aux augmentations, le fait d'itérer plusieurs fois permet \"d'augmenter\" artificiellement le nombre d'échantillon vus durant l'entrainement. Une boucle d'entrainement a la structure suivante:\n",
    "\n",
    "```python\n",
    "for epoch in epochs: # entrainement sur un nombre d'epochs donné\n",
    "\n",
    "    for X_batch, y_batch in zip(X_train, y_train): # on parcourt les données d'entrainement par batch\n",
    "\n",
    "        # prediction\n",
    "        y_pred = model.predict(X_batch)\n",
    "\n",
    "        # calcul de la perte\n",
    "        loss = model.loss(y_pred, y_batch)\n",
    "\n",
    "        # mise à jour des poids. Pour l'exemple on considère que la méthode\n",
    "        # qui met à jour les poids s'appelle \"update\" et prend \n",
    "        # en argument la perte\n",
    "        model.update_weights(loss)\n",
    "```\n",
    "\n",
    "Dans les faits, on peut directement utiliser la méthode `train` qui fait ces trois opérations directement. A noter que dans l'implémentation actuelle, les batch ont finalement une taille de 1, vu qu'on prend un échantillon à la fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on entraine le modèle pendant 3 epochs\n",
    "# on définit le learning_rate à 0.01\n",
    "\n",
    "learning_rate = 0.01\n",
    "N_epochs = 3\n",
    "\n",
    "# définition de la boucle d'entrainement\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    print('--- Epoch', epoch+1, '/', N_epochs ,'---')\n",
    "    ### A compléter \n",
    "    \n",
    "\n",
    "# maintenant que notre modèle est entrainé, on peut afficher une de ses prédictions\n",
    "\n",
    "plot_prediction(model, sample_idx=np.random.randint(0,X_test.shape[0]))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle feed-forward multi-couches\n",
    "\n",
    "#### Modèle à une couche \n",
    "\n",
    "On va maintenant implémenter un modèle avec une couche cachée et une activation sigmoide. Ce modèle sera entrainé par rétropropagation. Avant de commencer, on définit des fonctions `sigmoid` et sa dérivée `dsigmoid`, qui nous seront utiles pour la suite. \n",
    "\n",
    "Du point de vue des notations, on va noter $W_h$ et $b_h$ les poids de la couche cachée et $W_o$ et $b_o$ ceux de la dernière couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def dsigmoid(X):\n",
    "    '''Return the derivative of the sigmoid.'''\n",
    "    sig = sigmoid(X)\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    \"\"\"\n",
    "    MLP avec une couche cachée\n",
    "    Dans l'initialisation, on peut définir la taille de la couche intermédiaire.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # \n",
    "        #\n",
    "        # Définir des attributs (4 en tout), sur le modèle de la régression sigmoide\n",
    "        #\n",
    "        #\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, X, keep_activations=False):\n",
    "        \"\"\"\n",
    "        Dans la fonction forward, on définit une option `keep_activations`\n",
    "        qui renvoie les prédictions de la couche cachée en plus des prédictions\n",
    "        de la dernière couche. \n",
    "\n",
    "        Cette option est activée quand on calcule les gradients.\n",
    "        \"\"\"\n",
    "\n",
    "        #\n",
    "        # On définit la valeur du produit W * X + b \n",
    "        # et sa sigmoi\n",
    "        #\n",
    "\n",
    "        # Couche cachée\n",
    "        z_h = ### A compléter\n",
    "        h = softmax(z_h)\n",
    "\n",
    "        # Dernière couche, même modèle que pour\n",
    "        # la couche cachée\n",
    "\n",
    "        if keep_activations:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        y, h, z_h = self.forward(x, keep_activations=True)\n",
    "        \n",
    "        grad_z_o = ### A compléter\n",
    "        grad_W_o = ### A compléter\n",
    "        grad_b_o = ### A compléter\n",
    "        \n",
    "        # Maintenant que l'on a une chouche cachée, il faut aussi \n",
    "        # calculer ses gradients\n",
    "        grad_h   = ### A compléter \n",
    "        grad_z_h = ### A compléter - on fait intervenir ici la fonction dsigmoid \n",
    "        grad_W_h = ### A compléter\n",
    "        grad_b_h = ### A compléter\n",
    "\n",
    "        # on stocke les résultats dans un dictionnaire\n",
    "        \n",
    "        grads    = {\"W_h\": grad_W_h, \"b_h\": grad_b_h, \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        La structure est la même que pour la régression logistique\n",
    "        \"\"\"\n",
    "        #\n",
    "        # A compléter\n",
    "        #\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis = 1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis = 1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on initialise un modèle et on fait une prédiction sans entrainement\n",
    "\n",
    "n_hidden = 32 # Play with this parameter\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)\n",
    "\n",
    "\n",
    "plot_prediction(model, sample_idx=np.random.randint(0,X_test.shape[0]))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va pouvoir entrainer notre modèle. La structure est la même que précédemment. \n",
    "Durant l'entrainement, on va récupérer les pertes, que l'on affichera ensuite. Certaines librairies comme [Tensorboard](https://www.tensorflow.org/tensorboard) permettent de voir en direct l'entrainement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 100\n",
    "\n",
    "model = NeuralNet(n_features, n_hidden, n_classes) # on initialise un nouveau modèle\n",
    "\n",
    "losses, accuracies, accuracies_test = [], [], [] # listes que l'on affichera à la fin\n",
    "\n",
    "# on ajoute les pertes avant l'entrainement\n",
    "losses.append(model.loss(X_train, y_train)) \n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "\n",
    "    # A la fin de chaque epoch, on calcule la loss du modèle\n",
    "    # et on stocke ça dans les listes qui seront affichées à la fin\n",
    "    # de l'entrainement.\n",
    "    \n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "    if epoch%10 == 0: # Afficher périodiquement les pertes\n",
    "        print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "# on affiche l'accuracy sur le train et le test\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\", fontsize=20)\n",
    "plt.legend(fontsize=25, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on affiche un exemple de prédiction avec le modèle entrainé\n",
    "\n",
    "plot_prediction(model, sample_idx=np.random.randint(0,X_test.shape[0]))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un modèle avec deux couches cachées\n",
    "\n",
    "On reprend la même architecture, mais cette fois-ci avec deux couches cachées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayersNeuralNet():\n",
    "    \"\"\"\n",
    "    MLP avec deux couches cachées\n",
    "    Dans l'initialisation, on peut définir la taille de la couche intermédiaire.\n",
    "    \"\"\"\n",
    "\n",
    "    #\n",
    "    # Reprendre l'exemple du modèle à une couche\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement du modèle\n",
    "\n",
    "N_epochs = 100\n",
    "\n",
    "n_hidden_1 = 32\n",
    "n_hidden_2 = 16\n",
    "\n",
    "\n",
    "model = TwoLayersNeuralNet(n_features, n_hidden_1, n_hidden_2, n_classes) # on initialise un nouveau modèle\n",
    "\n",
    "losses, accuracies, accuracies_test = [], [], [] # listes que l'on affichera à la fin\n",
    "\n",
    "# on ajoute les pertes avant l'entrainement\n",
    "losses.append(model.loss(X_train, y_train)) \n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "    if epoch%10 == 0: # Afficher périodiquement les pertes\n",
    "        print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "# on affiche l'accuracy sur le train et le test\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\", fontsize=20)\n",
    "plt.legend(fontsize=25, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration du modèle\n",
    "\n",
    "Dans cette dernière section, libre à vous d'explorer un peu le modèle. Vous pouvez par exemple:\n",
    "\n",
    "* Regarder les pires prédictions du modèle avec la fonction `plot_prediction`\n",
    "* Ajuster les hyperparamètres (le learning rate, la taille de la couche cachée, l'initialisation)\n",
    "* Implémenter d'autres activations, par exemple la fonction `relu` au lieu de la fonction `sigmoid`\n",
    "* Vous pouvez enfin modifier le code pour que l'entrainement soit fait non plus par donnée mais par batch de données.\n",
    "* On peut également créer une classe qui permet d'initialiser un modèle avec `n` couches cachées. \n",
    "\n",
    "```python\n",
    "# un exemple de code pour récupérer les pires prédictions du modèle\n",
    "\n",
    "test_losses = -np.sum(np.log(EPSILON + model.forward(X_test)) * one_hot(10, y_test), axis=1)\n",
    "\n",
    "# Sort by ascending loss: best predictions first, worst at the end\n",
    "ranked_by_loss = test_losses.argsort()\n",
    "```\n",
    "\n",
    "Enfin, si vous voulez avoir des exemples visuels de réseaux de neurones, vous pouvez aller sur https://playground.tensorflow.org "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe214ecfce0b87c57a362cc23dc898772443e26f8754ada0c142e4895ea3525"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
