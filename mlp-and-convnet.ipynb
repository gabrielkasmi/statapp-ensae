{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation d'un réseau convolutif\n",
    "\n",
    "Dans ce second TD, on va partir de ce qui a été fait dans le premier TD pour implémenter un réseau convolutif. L'objectif de ce TD sera d'implémenter manuellement un [LeNet5](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) et de l'entrainer sur MNIST. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librairies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "# import mnist # package à installer conda install -c conda-forge mnist\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Jusqu'à présent, on a travaillé sur une version \"simplifiée\" de MNIST. Les données d'entrée n'avaient qu'une dimension de 64. Dans la suite, on va récupérer les \"vraies\" données de MNIST, i.e. des vignettes $28\\times28$. La dimensionnalité de l'espace d'entrée est désormais de 784. On peut récupérer les données directement avec la librairie `torchvision` de Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of columns must be a positive integer, not 8.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m batch_display \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(batch_display):\n\u001b[1;32m---> 43\u001b[0m     ax \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39;49madd_subplot(\u001b[39m2\u001b[39;49m, batch_display\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m, idx\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, xticks\u001b[39m=\u001b[39;49m[], yticks\u001b[39m=\u001b[39;49m[])\n\u001b[0;32m     44\u001b[0m     ax\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39msqueeze(images[idx]), cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m     \u001b[39m# print out the correct label for each image\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[39m# .item() gets the value contained in a Tensor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\lib\\site-packages\\matplotlib\\figure.py:772\u001b[0m, in \u001b[0;36mFigureBase.add_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m         args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m(args[\u001b[39m0\u001b[39m])))\n\u001b[0;32m    770\u001b[0m     projection_class, pkw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_projection_requirements(\n\u001b[0;32m    771\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 772\u001b[0m     ax \u001b[39m=\u001b[39m subplot_class_factory(projection_class)(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpkw)\n\u001b[0;32m    773\u001b[0m     key \u001b[39m=\u001b[39m (projection_class, pkw)\n\u001b[0;32m    774\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_axes_internal(ax, key)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_subplots.py:36\u001b[0m, in \u001b[0;36mSubplotBase.__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axes_class\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, fig, [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m \u001b[39m# This will also update the axes position.\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_subplotspec(SubplotSpec\u001b[39m.\u001b[39;49m_from_subplot_args(fig, args))\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\lib\\site-packages\\matplotlib\\gridspec.py:597\u001b[0m, in \u001b[0;36mSubplotSpec._from_subplot_args\u001b[1;34m(figure, args)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    594\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msubplot() takes 1 or 3 positional arguments but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(args)\u001b[39m}\u001b[39;00m\u001b[39m were given\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 597\u001b[0m gs \u001b[39m=\u001b[39m GridSpec\u001b[39m.\u001b[39;49m_check_gridspec_exists(figure, rows, cols)\n\u001b[0;32m    598\u001b[0m \u001b[39mif\u001b[39;00m gs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    599\u001b[0m     gs \u001b[39m=\u001b[39m GridSpec(rows, cols, figure\u001b[39m=\u001b[39mfigure)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\lib\\site-packages\\matplotlib\\gridspec.py:225\u001b[0m, in \u001b[0;36mGridSpecBase._check_gridspec_exists\u001b[1;34m(figure, nrows, ncols)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[39mreturn\u001b[39;00m gs\n\u001b[0;32m    224\u001b[0m \u001b[39m# else gridspec not found:\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m \u001b[39mreturn\u001b[39;00m GridSpec(nrows, ncols, figure\u001b[39m=\u001b[39;49mfigure)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\lib\\site-packages\\matplotlib\\gridspec.py:385\u001b[0m, in \u001b[0;36mGridSpec.__init__\u001b[1;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhspace \u001b[39m=\u001b[39m hspace\n\u001b[0;32m    383\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure \u001b[39m=\u001b[39m figure\n\u001b[1;32m--> 385\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(nrows, ncols,\n\u001b[0;32m    386\u001b[0m                  width_ratios\u001b[39m=\u001b[39;49mwidth_ratios,\n\u001b[0;32m    387\u001b[0m                  height_ratios\u001b[39m=\u001b[39;49mheight_ratios)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\lib\\site-packages\\matplotlib\\gridspec.py:52\u001b[0m, in \u001b[0;36mGridSpecBase.__init__\u001b[1;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     50\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of rows must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mnrows\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ncols, Integral) \u001b[39mor\u001b[39;00m ncols \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of columns must be a positive integer, not \u001b[39m\u001b[39m{\u001b[39;00mncols\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nrows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ncols \u001b[39m=\u001b[39m nrows, ncols\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_height_ratios(height_ratios)\n",
      "\u001b[1;31mValueError\u001b[0m: Number of columns must be a positive integer, not 8.0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2500x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 256\n",
    "\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2# convert data to torch.FloatTensor\n",
    "\n",
    "transform = transforms.ToTensor()# choose the training and testing datasets\n",
    "\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "\n",
    "indices = list(range(num_train))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_index, valid_index = indices[split:], indices[:split]# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, \n",
    "                                           sampler = train_sampler, num_workers = num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
    "                                          sampler = valid_sampler, num_workers = num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size,\n",
    "                                         num_workers = num_workers)\n",
    "\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter) #next.dataiter()\n",
    "images = images.numpy()# plot the images in the batch, along with the corresponding labels\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "batch_display = 16\n",
    "\n",
    "for idx in np.arange(batch_display):\n",
    "    ax = fig.add_subplot(2, batch_display/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition d'un réseau avec Pytorch\n",
    "\n",
    "Dans la fin du TD précédent, on a vu que l'implémentation d'un réseau de neurones pouvait rapidement être fastidieuse. Pour standardiser et faciliter le développement de réseaux de neurones, il existe un certain nombre de librairies. Les plus populaires actuellement sont Tensorflow et Pytorch. Ces librairies permettent non seulement de faciliter et d'optimiser l'implémentation de réseaux de neurones, mais prennent aussi en charge la parallélisation sur GPU de manière assez simple.\n",
    "\n",
    "### Réseau multi-couche\n",
    "\n",
    "On va réimplémenter, mais avec Pytorch, un réseau de neurones multicouche. On modifie la fonction d'activation pour passer au relu. Cette fonction d'activation présente l'avantage d'éviter les phénomènes de [disparition de gradient](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484) lors de l'entrainement.\n",
    "\n",
    "Dans l'exemple ci-dessous, on définit un attribut `layers` qui rassemble toutes les couches du modèle. L'utilisateur spécifie seulement la taille des couches cachées. Au sein de l'attribut `layers`, on utilise le module `Sequential` qui permet de \"cascader\" les différentes transformations qu'on applique aux inputs. L'attribut `nn.Linear` correspond à une couche complètement connectée. La non linéarité est ensuite appliquée en appelant la fonction `nn.ReLU`.\n",
    "\n",
    "#### Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# define NN architecture\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMLP\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, hidden_1, hidden_2):\n\u001b[0;32m      5\u001b[0m         \u001b[39msuper\u001b[39m(MLP,\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# define NN architecture\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_1, hidden_2):\n",
    "        super(MLP,self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features = 28*28, out_features = hidden_1, bias=True), # on explicite les arguments que prend la fonction nn.Linear\n",
    "            nn.ReLU(),\n",
    "            # Ajouter convolution?\n",
    "            # A vous de compléter le réseau\n",
    "            #\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1,28*28)\n",
    "        return self.layers(x)\n",
    "        \n",
    "# initialize the model, the loss function and the optimizer\n",
    "model = MLP(120,84)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement\n",
    "\n",
    "Dans la cellule ci-dessous, on met en place la boucle d'entrainement. On peut remarquer les similarités avec ce qui a été fait précédemment. Attention, en fonction des paramètres spécifiés et des caractéristiques de votre machine, cette cellule peut prendre du temps à être exécutée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 15# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "# lists to be plotted at the end of the training\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "\n",
    "    for data,label in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()  # prep model for evaluation\n",
    "    \n",
    "    for data,label in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # update running validation loss \n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.sampler) \n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss) \n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "\n",
    "# plot of the losses\n",
    "plt.plot([*range(len(train_losses))], train_losses / np.sum(train_losses), label = \"Train\")\n",
    "plt.plot([*range(len(train_losses))], val_losses / np.sum(val_losses), label = \"Val\")\n",
    "plt.xlabel('Training epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Maintenant que le modèle est entrainé, on l'évalue sur le test set et on affiche quelques exemples de prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of class %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        #print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "        print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "print(\"\"\"\n",
    "Overall accuracy {:0.2f} %\n",
    "\"\"\".format(np.sum(class_correct) / np.sum(class_total) * 100))\n",
    "\n",
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que le modèle atteint de bonnes performances (90% environ). Maintenant, on va définir un CNN et l'entrainer lui aussi sur MNIST.\n",
    "\n",
    "### Définition d'un réseau convolutif\n",
    "\n",
    "L'exemple ci-dessus a permis d'avoir un premier exemple du fonctionnement de Pytorch. Dans la suite, vous allez utiliser ce modèle pour implémenter vous-même un LeNet5. Jusqu'à présent, nous avons vu un type de couche : la couche complètement connectée ou <i> fully connected </i>. Ces couches sont notées $Fx$, où $x$ correspond à l'indice de la couche. \n",
    "Avec les réseaux convolutifs, on peut introduire deux nouveaux types de couches : \n",
    "* Les couches convolutives (notées $Cx$)\n",
    "* Les couches de pooling ou de subsampling (notées $Sx$)\n",
    "\n",
    "Ces couches sont définies dans le livre [DeepMaths](https://exo7math.github.io/deepmath-exo7/livre-deepmath.pdf) (p. 236 et suivantes pour les couches convolutives, p. 207 pour le subsampling).\n",
    "\n",
    "\n",
    "#### Les couches convolutives\n",
    "\n",
    "Pour implémenter une couche convolutive avec Pytorch, on utilise la fonction `Conv2D` du module `nn`. Les arguments de cette couche sont le nombre de canaux en entrée, `in_channels`, le nombre de canaux en sortie `out_channels`, la taille du noyaux `kernel_size` et la taille du stride, `stride`. Le `stride` est le pas (en pixels) appliqué au noyau. Pour un stride de 1, alors le noyau se déplace d'un pixel vers la droite, puis une fois la ligne terminée d'un pixel vers le bas. Un dernier paramètre est le `padding` qui correspond à un \"agrandissement\" artificiel de l'image lorsque le noyau passe sur les pixels au bord de l'image. \n",
    "\n",
    "On parle parfois de <i> feature map </i> pour désigner un ensemble d'unités de la couche convolutive pour lesquelles les poids sont identiques. Les features maps correspondent aux sorties de la couche convolutive. Dans l'exemple p.236 de DeepMaths, les features maps sont les filtres $M_1,\\dots M_32$. \n",
    "\n",
    "#### Les couches de subsampling\n",
    "\n",
    "Les couches de subsampling ou de pooling permettent de réduire la dimension des entrées. Le principe est de définir un noyau sur lequel on applique une opération de pooling (généralement moyenne ou max).\n",
    "\n",
    "#### Les couches d'aplatissement\n",
    "\n",
    "Ces couches consistent à reformater la sortie d'une couche convolutive (par exemple un tenseur $10\\times10\\times6$) en un vecteur de dimension 600. On utilise la fonction `nn.Flatten` pour faire cet aplatissement. \n",
    "\n",
    "#### L'architecture à implémenter\n",
    "\n",
    "Vous pouvez retrouver cette architecture sur la figure 2 du papier accessible [au lien suivant](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). Dans notre implémentation, on se propose de découper le modèle en deux parties: un <i> feature extractor </i> et un <i> classifier </i> classique. On définit donc deux méthodes, `self.feature_extractor` et `self.classifier`. Les paramètres pour le feature extractor sont les suivants:\n",
    "\n",
    "* Une couche $C_1$ qui consiste en la définition de 6 features maps (argument `out_channels`). Le noyau a une taille de 5, le stride est de 1 et le padding de 2. La couche se termine par une nonlinéarité `tanh`\n",
    "* Une couche $S_2$ est une couche de pooling (voir [ici](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) pour la fonction à utiliser). Le kernel est de 2 et le padding de 2.\n",
    "* Une couche $C_3$ est une couche qui prend les 6 features maps de $S_2$ en entrée et renvoie 16 features maps. Le noyau a une taille de 5, le stride est de 1. Il n'y a pas de padding.  La couche se termine par une nonlinéarité `tanh`\n",
    "* Une couche $S_4$ est à nouveau une couche de pooling.\n",
    "* Une couche $C_5$ est une dernière couche qui renvoie 120 feature maps. Le noyeau de cette couche est de taille 5. La couche se termine par une nonlinéarité `tanh`\n",
    "\n",
    "A la suite de ces couches convolutives, on applique un réseau à deux couches classique. La dimension d'entrée est 120 (contre 28*28 pour les données source). La dimension de la couche cachée est 84 et la dimension de sortie 10. \n",
    "\n",
    "<i> Remarque </i> \n",
    "\n",
    "Vous pourrez trouver certaines implémentations avec une taille d'input valant 32*32 au lieu de 28*28. Dans ce cas, il n'y a pas de padding appliqué. L'agrandissement de la taille de l'input permet d'avoir des features maps d'une taille 28*28 (ce que permet de manière équivalente le padding de 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # Première partie du modèle, qui correspond aux couches convolutionnelles.\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "\n",
    "            #\n",
    "            # A vous de compléter le réseau\n",
    "            #\n",
    "\n",
    "            nn.Flatten(start_dim = 1, end_dim = -1), # on applique un applatissement à la dernière couche pour renvoyer un vecteur [batch_size, 120] au classifieur\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            #\n",
    "            # A compléter\n",
    "            #\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model, the loss and the criterion\n",
    "\n",
    "n_classes = 10\n",
    "model = LeNet5(n_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 15# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "# lists to be plotted at the end of the training\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # monitor losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "\n",
    "    for data,label in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()  # prep model for evaluation\n",
    "    \n",
    "    for data,label in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,label)\n",
    "        # update running validation loss \n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_loader.sampler) \n",
    "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss) \n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "\n",
    "# plot of the losses\n",
    "plt.plot([*range(len(train_losses))], train_losses / np.sum(train_losses), label = \"Train\")\n",
    "plt.plot([*range(len(train_losses))], val_losses / np.sum(val_losses), label = \"Val\")\n",
    "plt.xlabel('Training epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.sampler)\n",
    "\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of class %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        #print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "        print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "print(\"\"\"\n",
    "Overall accuracy {:0.2f} %\n",
    "\"\"\".format(np.sum(class_correct) / np.sum(class_total) * 100))\n",
    "\n",
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds = torch.max(output, 1)\n",
    "# prep images for display\n",
    "images = images.numpy()# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration \n",
    "\n",
    "Maintenant, c'est à vous d'explorer le modèle et ses propriétés ! Vous pouvez jouer sur un grand nombre d'hyperparamètres: taille des noyaux, nombre de features maps, stride, padding, etc. Vous pouvez également comparer à extracteur de feature équivalent à quel point le réseau convolutif s'en sort mieux (ou pas). On peut également compter le nombre de paramètres à apprendre par rapport à un réseau complètement connecté de profondeur équivalente.\n",
    "\n",
    "Pour aller plus loin, vous pouvez également explorer la forme des features maps qui ont été apprises par le modèle et les comparer avec des filtres \"usuels\" en computer vision. Vous pouvez également comparer les représentations en extranyant les vecteurs issus de la première couche cachée du MLP avec ceux issus des couches convolutives du LeNet5. Pour pouvoir visualiser ces vecteurs, vous pouvez faire une ACP ou bien utiliser l'algorithme du [t-SNE](https://fr.wikipedia.org/wiki/Algorithme_t-SNE). N'hésitez pas à me solliciter si vous souhaitez avec plus d'informations sur comment faire pour aller 'explorer' les modèles. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d954cd638a1e7c86344e891e12f64c2dae4614b72a421e6b381622588a12ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
